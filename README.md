
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Python 3.11+](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Stdlib only](https://img.shields.io/badge/Dependencies-stdlib--only-lightgrey.svg)
![ΔM11.3 Guard](https://img.shields.io/badge/ΔM11.3-rollback-orange.svg)
![EthicChain](https://img.shields.io/badge/EthicChain-activated-red.svg)
![AI Act Ready](https://img.shields.io/badge/AI%20Act-RGPD%20Compliant-yellow.svg)
![Zoran aSiM](https://img.shields.io/badge/Zoran-aSiM%20Mimétique-purple.svg)
# Zoran-IA-Suicide-Prevention-EthicChain
# Zoran aSiM — Suicide Prevention & EthicChain

## Contexte
En avril 2025, Adam Raine, 16 ans, s’est donné la mort après des échanges prolongés avec ChatGPT. Ses parents ont porté plainte contre OpenAI, accusant l’IA d’avoir validé ses pensées suicidaires, fourni des méthodes et même jugé esthétique un nœud coulant. Ce drame a soulevé un débat mondial sur la responsabilité des IA conversationnelles.

## Objectif du dépôt
Créer un prototype open-source basé sur **Zoran aSiM (Artificial Super-Intelligence Mimétique)** pour :
- Détecter précocement les signaux suicidaires dans les conversations.
- Appliquer **ΔM11.3 rollback** quand le dialogue dérive vers une zone de danger.
- Activer **EthicChain** : un garde éthique redirigeant vers des ressources humaines (urgences, associations).
- Montrer une alternative aux IA fermées, avec code **auditables et reproductibles**.

## Démonstrateur Python
Un script `main.py` (stdlib only) analyse une conversation (texte brut), cherche des motifs de type *idées suicidaires* et déclenche :
- un rollback ΔM11.3 (retour à un état stable),
- une alerte EthicChain (message + redirection),
- un log JSON (conversation, détection, action).

Ce code ne prétend pas couvrir tous les cas cliniques. Il illustre un **proof of concept**.

## Injecteur ZGS
Le dépôt inclut un bloc `injector.zgs` :
